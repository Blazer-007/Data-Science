{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Processing_NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqlmyYQ5MPwKRo0+qPXeUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blazer-007/Data-Science/blob/master/Text_Processing_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efEubjZVOKZN",
        "colab_type": "text"
      },
      "source": [
        "# **Text Preprocessing using NLTK**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Install NLTK</br>\n",
        "     !pip install nltk\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOPXXkg2LITs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7uNXbAQVKy",
        "colab_type": "text"
      },
      "source": [
        "<b>NLTK contains many corpus or text datasets to work with it.</b></br>\n",
        "e.g. brown dataset , etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM-u6vdkNvGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nltk.download('brown')\n",
        "# from nltk.corpus import brown\n",
        "# print(brown.categories())\n",
        "# print(len(brown.categories()))\n",
        "# data = brown.sents(categories='adventure')\n",
        "# data\n",
        "# len(data)\n",
        "# data[0]\n",
        "# ' '.join(data[1]) ## to print sentence in adventure category"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UFSoEh4RC-a",
        "colab_type": "text"
      },
      "source": [
        "# Basic NLP Pipeline\n",
        "- Data Collection\n",
        "- Tokenization, Stopword, Stemming\n",
        "- Building a common vocab\n",
        "- Vectorizing the documents\n",
        "- Performing Classification/ Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jf2uEgNRun5",
        "colab_type": "text"
      },
      "source": [
        "### 2. Tokenization and Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn_XVm92PIrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers. I went to the market to buy some fruits. \"\"\"\n",
        "sentence = \"Send all the documents realated to SSD chapters 9,10,11 at vkr@ece.com\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDdXVEWbT9XX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "2340d654-485b-449d-aac8-bcdc559fb28a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXBZJenITl6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize,word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6JLW_JRTtjx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c6adf061-b6fd-4082-a826-8685c75d7026"
      },
      "source": [
        "# sent_tokenize() is used to convert a multi line string(document/paragraph) into list of strings\n",
        "sents = sent_tokenize(document)\n",
        "print(sents)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to buy some fruits.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5GXwwHIT3xH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37708de8-54b9-4598-8340-8047e0aea306"
      },
      "source": [
        "sents[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It was a very pleasant day.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu7QysoBUaWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To convert sentence into words two methods are present -\n",
        "# 1. inbuilt split() method\n",
        "# 2. word_tokenize() from nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XAqQVc8Us-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "4483c99a-9cb8-40e0-9443-8418cf5e7ac3"
      },
      "source": [
        "sentence.split()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Send',\n",
              " 'all',\n",
              " 'the',\n",
              " 'documents',\n",
              " 'realated',\n",
              " 'to',\n",
              " 'SSD',\n",
              " 'chapters',\n",
              " '9,10,11',\n",
              " 'at',\n",
              " 'vkr@ece.com']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEjZ94odUVuD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "869515af-a02b-4cdc-8d95-792744fbfbb1"
      },
      "source": [
        "word_list = word_tokenize(sentence)\n",
        "print(word_list)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Send', 'all', 'the', 'documents', 'realated', 'to', 'SSD', 'chapters', '9,10,11', 'at', 'vkr', '@', 'ece.com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk8uz85EVO_Q",
        "colab_type": "text"
      },
      "source": [
        "### Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knSY-c6vVjWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "949d1b48-672c-4f71-fb78-97dec4a900c3"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGFXNqWuVN5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KJ6-vmVU9pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sw  = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "993FnBbwVekB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e3966091-3660-448c-efe5-d8a158dac7d0"
      },
      "source": [
        "print(sw)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'she', 'and', \"you've\", 'while', 'below', 'until', 'after', \"she's\", 'for', 'it', 'nor', 'myself', \"shouldn't\", 'd', 'did', 'my', 'such', 'haven', 'few', 'whom', 'weren', 'of', 'again', 'most', 'themselves', 'our', 'o', 'that', 'off', \"didn't\", 'been', 'be', 'from', 'yourselves', 'me', 'ourselves', 'no', \"mustn't\", 'any', \"don't\", 'own', 'doing', 'll', 'to', 'needn', 'all', 'didn', \"won't\", 'once', 'who', 'if', 'the', 'had', 'more', 'other', 's', 'wasn', 'they', 'an', \"should've\", 'were', 'hers', 'now', 'both', 'is', 'he', 'before', 'shan', 'then', 'am', 'wouldn', 'as', 'above', 'have', 'her', 'at', 'them', 'on', 'over', \"mightn't\", 'during', 'yourself', 'your', 'mightn', 'against', 'with', \"wasn't\", 'can', 'do', 'than', 'his', 'himself', 'which', 'was', \"you'll\", 've', 'here', 'you', \"that'll\", 'each', 'theirs', 'isn', 'will', 'does', 'just', 'itself', 'aren', 'shouldn', 'very', 'those', 'having', \"wouldn't\", 'has', 'mustn', 'what', 'ain', \"you're\", 'because', 'or', 'couldn', \"couldn't\", \"doesn't\", 'are', 'ours', \"needn't\", 'into', 'we', 'their', 're', \"weren't\", \"aren't\", 'too', 'y', 'why', 'so', 'there', \"isn't\", 'doesn', 'yours', 'about', 'm', \"you'd\", 'further', 'only', 'hasn', 'where', 'this', 'not', 'should', 'out', \"hadn't\", 'him', 'between', 'in', 'i', \"hasn't\", 'some', 'don', 'through', \"shan't\", 'up', 'hadn', 't', 'won', 'its', 'but', 'how', 'same', \"it's\", 'when', \"haven't\", 'herself', 'these', 'a', 'by', 'down', 'under', 'being', 'ma'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTUiWXatWFCQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stopwords(word_list):\n",
        "    useful_words = [w for w in word_list if w not in sw]\n",
        "    return useful_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7HagJ5qVoNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "11f1906a-20f3-4a78-d413-5d8008dbe638"
      },
      "source": [
        "useful_words= remove_stopwords(word_tokenize(sents[1]))\n",
        "print(useful_words)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['The', 'weather', 'cool', 'light', 'showers', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLnBGtulWWKx",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization using Regular Expression \n",
        "Problem with Word Tokenizer - Doesn't remove numbers and Can't handle complex tokenizations ! So we use a Regexp Tokenizer Class in NLTK\n",
        "\n",
        "For help in writing RegExpression , see cheatsheets at [Regex Tester](https://www.regexpal.com/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOgr6m1aV0kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSaOaYQjWrzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "106a3c1b-ae42-420a-8f51-16ff65bad837"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[a-zA-Z]+\")    # expression inside [] will be used in tokenization and + is used to take whole word as one token\n",
        "useful_text = tokenizer.tokenize(sentence)\n",
        "useful_text"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Send',\n",
              " 'all',\n",
              " 'the',\n",
              " 'documents',\n",
              " 'realated',\n",
              " 'to',\n",
              " 'SSD',\n",
              " 'chapters',\n",
              " 'at',\n",
              " 'vkr',\n",
              " 'ece',\n",
              " 'com']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHpzbeg-W6L9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "37fdcf4f-5a32-47ae-dbce-1dd31e670c70"
      },
      "source": [
        "tokenizer = RegexpTokenizer(\"[a-zA-Z@]+\")\n",
        "useful_text = tokenizer.tokenize(sentence)\n",
        "useful_text"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Send',\n",
              " 'all',\n",
              " 'the',\n",
              " 'documents',\n",
              " 'realated',\n",
              " 'to',\n",
              " 'SSD',\n",
              " 'chapters',\n",
              " 'at',\n",
              " 'vkr@ece',\n",
              " 'com']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wq6HR0tYYge",
        "colab_type": "text"
      },
      "source": [
        "## Stemming\n",
        "- Process that transforms particular words(verbs,plurals)into their radical form\n",
        "- Preserve the semantics of the sentence without increasing the number of unique tokens\n",
        "- jumps, jumping, jumped, jump ==> jump"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsnlu75IXKjv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffad4614-7395-434d-b9a4-516482d4b831"
      },
      "source": [
        "text= \"\"\"Foxes love to make jumps.The quick brown fox was seen jumping over the \n",
        "        lovely dog from a 6ft feet high wall\"\"\"\n",
        "\n",
        "words_list = tokenizer.tokenize(text.lower())\n",
        "print(words_list)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['foxes', 'love', 'to', 'make', 'jumps', 'the', 'quick', 'brown', 'fox', 'was', 'seen', 'jumping', 'over', 'the', 'lovely', 'dog', 'from', 'a', 'ft', 'feet', 'high', 'wall']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAdr-b3uYnnG",
        "colab_type": "text"
      },
      "source": [
        "### Stemming \n",
        "\n",
        "- 1) Snowball Stemmer (Multilingual)\n",
        "- 2) Porter Stemmer \n",
        "- 3) LancasterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnRaYXhDYiq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.snowball import PorterStemmer,SnowballStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px-jDBpqYs8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aswq8Vp_Yvp7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "efdc164d-1e6b-4d35-c844-6662774efa50"
      },
      "source": [
        "print(ps.stem('playing'))\n",
        "print(ps.stem('played'))\n",
        "print(ps.stem('plays'))\n",
        "print(ps.stem('play'))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwM2FobdY-Ry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ss = SnowballStemmer('english')      # Since it is multilingual stemmer , it is need to specify the language"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNh8QhhAZPwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a8451432-3c67-4904-97b8-e83f1d309466"
      },
      "source": [
        "print(ss.stem('playing'))\n",
        "print(ss.stem('played'))\n",
        "print(ss.stem('plays'))\n",
        "print(ss.stem('play'))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "play\n",
            "play\n",
            "play\n",
            "play\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk6c-CTbZd28",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "3b5927e2-f1e4-4e1a-cac7-229d02503a6a"
      },
      "source": [
        "print(ss.stem('lovely'))\n",
        "print(ss.stem('loving'))\n",
        "print(ss.stem('lover'))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "love\n",
            "love\n",
            "lover\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkXL406yZokP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e190387d-d111-444d-ebb1-777db1106a5d"
      },
      "source": [
        "print(ss.stem('fighting'))\n",
        "print(ss.stem('fight--ing'))   # if no base word is found , it just remove 'ing' "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fight\n",
            "fight--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Nj7y-BZ8z5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "68306422-76bf-499a-9582-705210e35f90"
      },
      "source": [
        "ls = LancasterStemmer()\n",
        "ls.stem(\"teeth\")\n",
        "\n",
        "print(ls.stem(\"teenager\")) #English\n",
        "print(ps.stem(\"teenager\")) #English\n",
        "print(ss.stem('teenager')) #English"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "teen\n",
            "teenag\n",
            "teenag\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuBBWILdap0X",
        "colab_type": "text"
      },
      "source": [
        "**All in One Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG_XwspqdBtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq_VoJcGaezC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_preprocess(text):\n",
        "  word_splits = tokenizer.tokenize(text) #regex tokenizer\n",
        "  useful_words= remove_stopwords(word_splits)\n",
        "  return [ls.stem(w) for w in useful_words ]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd37gCREcAMX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strr = \" I am playing cricket and after that i will watch cricket , then I will go for studying . \""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaYxBQ9KcWml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e6ba99e4-a008-4e8a-e59e-927e90c951f4"
      },
      "source": [
        "strrr = text_preprocess(strr)\n",
        "print(strrr) "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'play', 'cricket', 'watch', 'cricket', 'i', 'go', 'study']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsTVG4e1dyyW",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization\n",
        "Same as stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmftJTwLeAhz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "5d888746-3c85-4b7a-8a8b-4359fff8f1d3"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5d6wiiOchxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d58c840d-e30c-428c-ceb7-90e041716e67"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "l = WordNetLemmatizer()\n",
        "l.lemmatize(\"crying\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cry'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z_QeSa2eK0H",
        "colab_type": "text"
      },
      "source": [
        "# **Bag of Words Model Construction**\n",
        "\n",
        "## Building Common Vocab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqHUNmKHd9dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
        "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
        "        'The nobel laurate won the hearts of the people',\n",
        "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MrkCCGzfE1e",
        "colab_type": "text"
      },
      "source": [
        "Here first we will find common words for vocabulary and make a dictionary to match every word with a number , which will help in converting documents into vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sYBft9ve7DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HehoK50PfvDu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATb2rB8Tf7ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_corpus = cv.fit_transform(corpus)  #fit will learn dictionary and transform will convert into vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpA3pKhDgMSU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1b7a0095-f8e1-4dc1-b7bd-9811ed26a973"
      },
      "source": [
        "vectorized_corpus  #It is a sparse matrix "
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<4x42 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 47 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY7UwvATgTfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_corpus = vectorized_corpus.toarray() #converting sparse matrix into array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqncvo0wgTcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "9097ba93-484e-4d9a-932c-de2e2da75449"
      },
      "source": [
        "print(vectorized_corpus[0]) #to see the first document\n",
        "print(len(vectorized_corpus[0]))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
            " 2 0 1 0 2]\n",
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehxTAxtwgTY2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7254ff33-6c77-4249-d815-4d2b98434f1b"
      },
      "source": [
        "print(cv.vocabulary_)    #matching b/w words and index where it will occur in vector"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'indian': 12, 'cricket': 6, 'team': 31, 'will': 37, 'wins': 39, 'world': 41, 'cup': 7, 'says': 27, 'capt': 4, 'virat': 35, 'kohli': 14, 'be': 3, 'held': 11, 'at': 1, 'sri': 29, 'lanka': 15, 'we': 36, 'win': 38, 'next': 19, 'lok': 17, 'sabha': 26, 'elections': 8, 'confident': 5, 'pm': 23, 'the': 32, 'nobel': 20, 'laurate': 16, 'won': 40, 'hearts': 10, 'of': 21, 'people': 22, 'movie': 18, 'raazi': 24, 'is': 13, 'an': 0, 'exciting': 9, 'spy': 28, 'thriller': 33, 'based': 2, 'upon': 34, 'real': 25, 'story': 30}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwtsSyIPgTQ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b40d7d1-da55-421e-85b9-f46038458a22"
      },
      "source": [
        "print(len(cv.vocabulary_.keys()))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbXlIwiPh6_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reverse mapping or inverse transform i.e. from vector to original sentence\n",
        "numbers = vectorized_corpus[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpJ_vCYPicGF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9d4e4022-153c-4edb-b60e-7d2cbd26bf62"
      },
      "source": [
        "numbers"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpkWG7vciOxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0587baf4-0280-4de1-e1de-9aa94d569808"
      },
      "source": [
        "s = cv.inverse_transform(numbers)\n",
        "print(s)     # It will not be in correct order that's why model is called Bag of Words Model"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
            "      dtype='<U9')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXW51Z0qjJDI",
        "colab_type": "text"
      },
      "source": [
        "## Vectorization with Stopwords removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3A6509yqigTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def myTokenizer(document):\n",
        "  words = tokenizer.tokenize(document.lower())\n",
        "  words = remove_stopwords(words)\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x85azF5anoqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "149e6048-3184-435c-fd9f-080880a1711a"
      },
      "source": [
        "myTokenizer(\"this is some FUNCTION\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['function']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCbSV6O3ntnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer(tokenizer=myTokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPRImQiHnth_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorized_corpus = cv.fit_transform(corpus).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4bvuprDnte2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "4eaf0398-eb51-44ce-f3a2-43963b920874"
      },
      "source": [
        "#vectorized_corpus\n",
        "print(vectorized_corpus)\n",
        "print(len(vectorized_corpus[0]))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 2]\n",
            " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0]]\n",
            "33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IayasSlntZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "4ce97264-e2fe-4721-fefc-075e1e48f36b"
      },
      "source": [
        "cv.inverse_transform(vectorized_corpus)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['capt', 'cricket', 'cup', 'held', 'indian', 'kohli', 'lanka',\n",
              "        'says', 'sri', 'team', 'virat', 'wins', 'world'], dtype='<U9'),\n",
              " array(['confident', 'elections', 'indian', 'lok', 'next', 'pm', 'sabha',\n",
              "        'says', 'win'], dtype='<U9'),\n",
              " array(['hearts', 'laurate', 'nobel', 'people'], dtype='<U9'),\n",
              " array(['based', 'exciting', 'indian', 'movie', 'raazi', 'real', 'spy',\n",
              "        'story', 'thriller', 'upon'], dtype='<U9')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQzgUkEmntMN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For test data\n",
        "test_corpus = [\n",
        "               'Cricket Rocks!',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWK-7iNZo6Ht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4408076-cb5a-4ecb-e7d5-e04f4943ca0f"
      },
      "source": [
        "vectorized_test_corpus = cv.transform(test_corpus).toarray()\n",
        "print(vectorized_test_corpus)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJx5mP1NpU0-",
        "colab_type": "text"
      },
      "source": [
        "### Features in Bag of Words Model\n",
        "- Unigrams\n",
        "- Bigrams, Trigrams\n",
        "- N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGkRRf_rpPCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_1 = [\"this is good movie\"]\n",
        "sent_2 = [\"this is not good movie\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOJ45ZxVpO_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv = CountVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq-V8DGYpO8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "89caf781-4c78-4a3f-b731-33a71de5131a"
      },
      "source": [
        "docs = [sent_1[0],sent_2[0]]\n",
        "cv.fit_transform(docs).toarray()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 0, 1],\n",
              "       [1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKvxvgt7pO4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Bigrams treats every pair as features it helps in capturing negation and more features\n",
        "#ngram_range = (1,1) -> Unigram\n",
        "#ngram_range = (2,2) -> Bigram\n",
        "#ngram_range = (3,3) -> Trigram\n",
        "#ngram_range = (1,n) -> ngram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRwJxkG7pO13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d6810378-83ff-4941-affb-2af4118201a8"
      },
      "source": [
        "cv = CountVectorizer(tokenizer=myTokenizer,ngram_range=(1,1))\n",
        "vectorized_corpus = cv.fit_transform(corpus)\n",
        "vc = vectorized_corpus.toarray()\n",
        "\n",
        "print(cv.vocabulary_)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'indian': 9, 'cricket': 3, 'team': 26, 'wins': 31, 'world': 32, 'cup': 4, 'says': 22, 'capt': 1, 'virat': 29, 'kohli': 10, 'held': 8, 'sri': 24, 'lanka': 11, 'win': 30, 'next': 15, 'lok': 13, 'sabha': 21, 'elections': 5, 'confident': 2, 'pm': 18, 'nobel': 16, 'laurate': 12, 'hearts': 7, 'people': 17, 'movie': 14, 'raazi': 19, 'exciting': 6, 'spy': 23, 'thriller': 27, 'based': 0, 'upon': 28, 'real': 20, 'story': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gVfB-wXq1Gl",
        "colab_type": "text"
      },
      "source": [
        "# Tf-idf Normalisation\n",
        "- Avoid features that occur very often, becauase they contain less information\n",
        "- Information decreases as the number of occurences increases across different type of documents\n",
        "- So we define another term - term-document-frequency which associates a weight with every term"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aNjRcCZrydr",
        "colab_type": "text"
      },
      "source": [
        "Tf-idf -> Term frequency - inverse document frequency</br>\n",
        "tf(t,d) -> this is count of particular term in a particular document </br>\n",
        "idf(t,d) = log(N/(1+count(t,D))) -> </br>\n",
        "where , N = total no of documents </br>\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp; count(t,d) = how many times t appears across all d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcwn6Ssaq9qV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_1 = \"this is good movie\"\n",
        "sent_2 = \"this was good movie\"\n",
        "sent_3 = \"this is not good movie\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbhOA1UWq-8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [sent_1,sent_2,sent_3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMbfKt4_q-zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZRcbelRq-pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBlI2nM6tPDP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "fc9a7379-03fe-43ca-ff99-1463bdbbf655"
      },
      "source": [
        "vectorized_corpus = tfidf.fit_transform(corpus).toarray()\n",
        "print(vectorized_corpus)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
            " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
            " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KELQhKt_tO5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e73ac03-7f58-430e-ae71-17305c97b959"
      },
      "source": [
        "print(tfidf.vocabulary_)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARo4oo0Cq-XR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqu5nNc5q-Ux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0952be02-15a2-4ab3-ff9a-b8310b1099c9"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=myTokenizer,ngram_range=(1,1),norm='l2')\n",
        "\n",
        "vectorized_corpus = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
        "print(vectorized_corpus)\n",
        "print(tfidf_vectorizer.vocabulary_)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.70710678 0.70710678]\n",
            " [0.70710678 0.70710678]\n",
            " [0.70710678 0.70710678]]\n",
            "{'good': 0, 'movie': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keLaFNTXxSNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}